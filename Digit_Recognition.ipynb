{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48d90849-6cbb-4900-b486-13298cf7ee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e596d3-f7af-4d45-823e-bf563aa28395",
   "metadata": {},
   "source": [
    "This code is written in Python and uses the PyTorch library to load the MNIST dataset for training and testing machine learning models. MNIST is a popular dataset of handwritten digits (0-9) commonly used for image classification tasks.\n",
    "\n",
    "Let's break down the code:\n",
    "\n",
    "1. `datasets.MNIST`: This is a class provided by PyTorch in the `torchvision.datasets` module specifically designed for handling the MNIST dataset.\n",
    "\n",
    "2. `root='data'`: Specifies the directory where the dataset will be downloaded. In this case, it's set to 'data'.\n",
    "\n",
    "3. `train=True` (for `train_data`) and `train=False` (for `test_data`): These parameters indicate whether the dataset should be the training set (`train=True`) or the testing set (`train=False`).\n",
    "\n",
    "4. `transform=ToTensor()`: This parameter applies a transformation to the data. In this case, it uses the `ToTensor()` transformation, which converts the images from PIL Image format to PyTorch tensors. PyTorch tensors are the basic building blocks for creating and training neural networks.\n",
    "\n",
    "5. `download=True`: This parameter specifies whether to download the dataset if it is not already present in the specified root directory.\n",
    "\n",
    "In summary, this code snippet is creating two datasets: `train_data` for training and `test_data` for testing, both based on the MNIST dataset. The images in these datasets will be converted to PyTorch tensors, making them suitable for training neural network models. If the dataset is not present in the 'data' directory, it will be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa88d2cf-bbae-4390-b8dd-161f0cd55837",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,\n",
    "    transform = ToTensor(),\n",
    "    download = True \n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = False,\n",
    "    transform = ToTensor(),\n",
    "    download = True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c06e786d-fd72-4d95-b6a4-cb08811169d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a06496e9-6b0b-43d3-a265-a8793545f749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7859b042-b779-4ed3-95ae-69391c6f4ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b40d20e5-e28c-4188-8343-cd1f972e39d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c63d7bd-cf90-4b57-8a89-0e74048c9061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4,  ..., 5, 6, 8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9f25e9-021f-409f-84d5-3404a3f23c7f",
   "metadata": {},
   "source": [
    "This code is setting up PyTorch `DataLoader` objects for the training and testing datasets created using the MNIST dataset. `DataLoader` is a PyTorch utility that helps efficiently load and iterate over batches of data during the training process.\n",
    "\n",
    "Let's break down the code:\n",
    "\n",
    "1. `from torch.utils.data import DataLoader`: Imports the `DataLoader` class from the PyTorch `torch.utils.data` module.\n",
    "\n",
    "2. `loaders`: This is a dictionary that will contain two `DataLoader` objects - one for the training data and one for the testing data.\n",
    "\n",
    "3. `\"train\"` and `\"test\"`: These are keys in the `loaders` dictionary, representing the training and testing data loaders, respectively.\n",
    "\n",
    "4. `DataLoader(train_data, batch_size=100, shuffle=True, num_workers=1)`: Creates a `DataLoader` for the training data (`train_data`). Let's break down the parameters:\n",
    "   - `train_data`: The dataset to be loaded.\n",
    "   - `batch_size=100`: Specifies the number of samples in each batch. In this case, each batch will contain 100 samples.\n",
    "   - `shuffle=True`: Randomly shuffles the data at the beginning of each epoch. Shuffling the data helps the model generalize better during training.\n",
    "   - `num_workers=1`: Number of subprocesses to use for data loading. Setting it to 1 means that the data loading will be performed in the main process.\n",
    "\n",
    "5. `DataLoader(test_data, batch_size=100, shuffle=True, num_workers=1)`: Similar to the training data loader, this creates a `DataLoader` for the testing data (`test_data`) with the same parameters.\n",
    "\n",
    "In summary, this code sets up two `DataLoader` objects, one for training and one for testing, each configured to load data in batches of 100, shuffle the data, and use a single worker process for loading data. These loaders can be used in the training and testing loops of a machine learning model to efficiently iterate over the data during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "048ec20c-d824-40b1-98fb-5054fa0f3456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loaders = {\n",
    "    \"train\": DataLoader(train_data,\n",
    "                        batch_size=100,\n",
    "                        shuffle=True,\n",
    "                        num_workers=1),\n",
    "    \n",
    "    \"test\": DataLoader(test_data,\n",
    "                       batch_size=100,\n",
    "                       shuffle=True,\n",
    "                       num_workers=1)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d430703d-c8a1-4a34-b643-feb862db07e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x19c1f158a50>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x19c1f15ae50>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235152d7-a25e-4b95-bb62-b178f27c9e7f",
   "metadata": {},
   "source": [
    "This code defines a convolutional neural network (CNN) using the PyTorch library. Let's break it down step by step:\n",
    "\n",
    "1. `import torch.nn as nn`: Imports the neural network module from the PyTorch library.\n",
    "\n",
    "2. `import torch.nn.functional as F`: Imports the functional interface for operations that don't have any parameters like weights.\n",
    "\n",
    "3. `import torch.optim as optim`: Imports the optimization module from PyTorch.\n",
    "\n",
    "4. `class CNN(nn.Module):`: Defines a class named `CNN` that inherits from the `nn.Module` class, which is the base class for all neural network modules in PyTorch.\n",
    "\n",
    "5. `def __init__(self):`: The constructor method for the `CNN` class. It initializes the layers of the neural network.\n",
    "\n",
    "   - `self.conv1 = nn.Conv2d(1, 10, kernel_size=5)`: Defines the first convolutional layer with 1 input channel, 10 output channels, and a kernel size of 5x5.\n",
    "   \n",
    "   - `self.conv2 = nn.Conv2d(10, 20, kernel_size=5)`: Defines the second convolutional layer with 10 input channels (from the previous layer), 20 output channels, and a kernel size of 5x5.\n",
    "   \n",
    "   - `self.conv2_drop = nn.Dropout2d()`: Defines a 2D dropout layer. Dropout is a regularization technique that randomly sets a fraction of input units to zero during training to prevent overfitting.\n",
    "   \n",
    "   - `self.fc1 = nn.Linear(320, 50)`: Defines the first fully connected (dense) layer with 320 input features and 50 output features.\n",
    "   \n",
    "   - `self.fc2 = nn.Linear(50, 10)`: Defines the second fully connected layer with 50 input features and 10 output features (corresponding to the 10 classes in the MNIST dataset).\n",
    "\n",
    "6. `def forward(self, x):`: Defines the forward pass of the neural network. This method is called when the model is passed an input `x`.\n",
    "\n",
    "   - `x = F.relu(F.max_pool2d(self.conv1(x), 2))`: Applies the first convolutional layer, followed by a ReLU activation function and max pooling with a kernel size of 2x2.\n",
    "   \n",
    "   - `x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))`: Applies the second convolutional layer, followed by a dropout, ReLU activation, and max pooling with a kernel size of 2x2.\n",
    "   \n",
    "   - `x = x.view(-1, 320)`: Flattens the tensor to prepare it for the fully connected layers.\n",
    "   \n",
    "   - `x = F.relu(self.fc1(x))`: Applies the first fully connected layer with a ReLU activation function.\n",
    "   \n",
    "   - `x = F.dropout(x, training=self.training)`: Applies dropout to the output of the first fully connected layer.\n",
    "   \n",
    "   - `x = self.fc2(x)`: Applies the second fully connected layer.\n",
    "   \n",
    "   - `return F.log_softmax(x, dim=1)`: Applies the log softmax function to the output, providing the final probability distribution over the classes.\n",
    "\n",
    "In summary, this code defines a simple CNN architecture for image classification, specifically designed for the MNIST dataset. It uses convolutional layers, max pooling, dropout, and fully connected layers. The forward method specifies how the input is processed through these layers to produce the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a20a6da-1dc1-4c96-91e3-2137b3549942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)  # Flatten the tensor for the fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743bfd74-891c-4189-b578-857b75a93ca4",
   "metadata": {},
   "source": [
    "This code is an example of training and testing a neural network using PyTorch. Let's break down the key components:\n",
    "\n",
    "1. **Device Configuration:**\n",
    "   ```python\n",
    "   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "   ```\n",
    "   - This line checks if a CUDA-enabled GPU is available. If so, it sets the device to 'cuda', otherwise to 'cpu'. This is useful for utilizing GPU acceleration if available.\n",
    "\n",
    "2. **Model Initialization:**\n",
    "   ```python\n",
    "   model = CNN().to(device)\n",
    "   ```\n",
    "   - It creates an instance of the CNN model and moves it to the specified device (either GPU or CPU).\n",
    "\n",
    "3. **Optimizer and Loss Function:**\n",
    "   ```python\n",
    "   optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "   loss_fn = nn.CrossEntropyLoss()\n",
    "   ```\n",
    "   - It sets up an Adam optimizer to update the model parameters during training, and uses cross-entropy loss as the criterion for training.\n",
    "\n",
    "4. **Training Function:**\n",
    "   ```python\n",
    "   def train(epoch):\n",
    "       # ...\n",
    "   ```\n",
    "   - The `train` function is responsible for training the model for a specified number of epochs.\n",
    "   - It iterates through batches of training data, performs forward and backward passes, calculates the loss, and updates the model weights using gradient descent.\n",
    "   - Training progress is printed every 20 batches.\n",
    "\n",
    "5. **Testing Function:**\n",
    "   ```python\n",
    "   def test():\n",
    "       # ...\n",
    "   ```\n",
    "   - The `test` function evaluates the trained model on the test dataset.\n",
    "   - It calculates the average test loss and the accuracy of the model on the test set.\n",
    "   - The model is put into evaluation mode (`model.eval()`), and the `torch.no_grad()` context is used to disable gradient computation during testing.\n",
    "\n",
    "6. **Data Loading:**\n",
    "   - The code assumes the existence of data loaders (`loaders['train']` and `loaders['test']`) that provide batches of training and testing data.\n",
    "\n",
    "7. **Training and Testing Loop:**\n",
    "   ```python\n",
    "   for epoch in range(num_epochs):\n",
    "       train(epoch)\n",
    "   test()\n",
    "   ```\n",
    "   - The model is trained for a specified number of epochs using the `train` function.\n",
    "   - After training, the `test` function is called to evaluate the model on the test set.\n",
    "\n",
    "This code demonstrates the standard structure of training and testing loops for a PyTorch neural network. It's assumed that the dataset and data loaders (`loaders['train']` and `loaders['test']`) are set up outside this code. Adjustments may be needed based on the specifics of your dataset and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39d41c1d-3acf-4adb-b5db-92d21c8b6b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Create an instance of the CNN model\n",
    "model = CNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "       # Print training progress\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{ batch_idx * len(data)}/{len(loaders['train'].dataset)} ({100. * batch_idx / len(loaders['train']):.0f}%)]\\t{loss.item():.6f}\")\n",
    "\n",
    "# Assuming you have a DataLoader named train_loader for your training data\n",
    "# and log_interval is a variable indicating how often to print training progress.\n",
    "# You'll need to set these values accordingly.\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in loaders['test']:  # Assuming you have a DataLoader named test_loader\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "\n",
    "            # Calculate the test loss\n",
    "            test_loss += loss_fn(output, target).item()\n",
    "\n",
    "            # Get the index of the max log-probability (predicted class)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    # Average test loss\n",
    "    test_loss /= len(loaders['test'].dataset)\n",
    "\n",
    "    print(f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(loaders['test'].dataset)} ({100.*correct/len(loaders['test'].dataset):.0f}%\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c04894-140e-411e-a8f5-a2ffddca7a5a",
   "metadata": {},
   "source": [
    "This code snippet is a simple training and testing loop that iterates over a specified number of epochs. It's designed to train a neural network using the `train` function and evaluate its performance on a test set using the `test` function. Here's the breakdown:\n",
    "\n",
    "```python\n",
    "for epoch in range(1, 11):\n",
    "    train(epoch)\n",
    "    test()\n",
    "```\n",
    "\n",
    "- **Loop Over Epochs:**\n",
    "  - `for epoch in range(1, 11):` creates a loop that iterates over the range from 1 to 10 (inclusive). This loop represents the training process over multiple epochs.\n",
    "\n",
    "- **Training:**\n",
    "  - `train(epoch)` is called inside the loop. This function is assumed to perform the training process for one epoch. The `epoch` variable is likely used within the `train` function to keep track of the current epoch.\n",
    "\n",
    "- **Testing:**\n",
    "  - `test()` is called immediately after the training for the current epoch. This function is responsible for evaluating the model's performance on a test set. It calculates the average test loss and the accuracy of the model on the test data.\n",
    "\n",
    "- **Iteration:**\n",
    "  - The loop continues to the next epoch, repeating the training and testing steps.\n",
    "\n",
    "In summary, this code runs a training loop for 10 epochs. In each epoch, it trains the model using the `train` function and then evaluates its performance on the test set using the `test` function. This is a common structure in deep learning training scripts to iteratively improve the model over multiple passes through the training data. Adjustments may be necessary depending on the specifics of your neural network architecture and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0843462c-4086-45b0-889e-3681f9fd5af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\t2.313310\n",
      "Train Epoch: 1 [2000/60000 (3%)]\t0.950831\n",
      "Train Epoch: 1 [4000/60000 (7%)]\t0.971186\n",
      "Train Epoch: 1 [6000/60000 (10%)]\t0.679772\n",
      "Train Epoch: 1 [8000/60000 (13%)]\t0.492723\n",
      "Train Epoch: 1 [10000/60000 (17%)]\t0.617950\n",
      "Train Epoch: 1 [12000/60000 (20%)]\t0.683835\n",
      "Train Epoch: 1 [14000/60000 (23%)]\t0.446239\n",
      "Train Epoch: 1 [16000/60000 (27%)]\t1.105363\n",
      "Train Epoch: 1 [18000/60000 (30%)]\t0.529517\n",
      "Train Epoch: 1 [20000/60000 (33%)]\t0.352600\n",
      "Train Epoch: 1 [22000/60000 (37%)]\t0.250608\n",
      "Train Epoch: 1 [24000/60000 (40%)]\t0.508317\n",
      "Train Epoch: 1 [26000/60000 (43%)]\t0.334604\n",
      "Train Epoch: 1 [28000/60000 (47%)]\t0.609673\n",
      "Train Epoch: 1 [30000/60000 (50%)]\t0.635350\n",
      "Train Epoch: 1 [32000/60000 (53%)]\t0.527652\n",
      "Train Epoch: 1 [34000/60000 (57%)]\t0.513418\n",
      "Train Epoch: 1 [36000/60000 (60%)]\t0.349972\n",
      "Train Epoch: 1 [38000/60000 (63%)]\t0.323529\n",
      "Train Epoch: 1 [40000/60000 (67%)]\t0.292243\n",
      "Train Epoch: 1 [42000/60000 (70%)]\t0.323808\n",
      "Train Epoch: 1 [44000/60000 (73%)]\t0.560108\n",
      "Train Epoch: 1 [46000/60000 (77%)]\t0.349672\n",
      "Train Epoch: 1 [48000/60000 (80%)]\t0.500854\n",
      "Train Epoch: 1 [50000/60000 (83%)]\t0.220359\n",
      "Train Epoch: 1 [52000/60000 (87%)]\t0.309879\n",
      "Train Epoch: 1 [54000/60000 (90%)]\t0.380531\n",
      "Train Epoch: 1 [56000/60000 (93%)]\t0.216119\n",
      "Train Epoch: 1 [58000/60000 (97%)]\t0.362511\n",
      "\n",
      "Test set: Average loss: 0.0014, Accuracy: 9594/10000 (96%\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\t0.308676\n",
      "Train Epoch: 2 [2000/60000 (3%)]\t0.146913\n",
      "Train Epoch: 2 [4000/60000 (7%)]\t0.349719\n",
      "Train Epoch: 2 [6000/60000 (10%)]\t0.341934\n",
      "Train Epoch: 2 [8000/60000 (13%)]\t0.300882\n",
      "Train Epoch: 2 [10000/60000 (17%)]\t0.524321\n",
      "Train Epoch: 2 [12000/60000 (20%)]\t0.358598\n",
      "Train Epoch: 2 [14000/60000 (23%)]\t0.255005\n",
      "Train Epoch: 2 [16000/60000 (27%)]\t0.277864\n",
      "Train Epoch: 2 [18000/60000 (30%)]\t0.394609\n",
      "Train Epoch: 2 [20000/60000 (33%)]\t0.458236\n",
      "Train Epoch: 2 [22000/60000 (37%)]\t0.260466\n",
      "Train Epoch: 2 [24000/60000 (40%)]\t0.409066\n",
      "Train Epoch: 2 [26000/60000 (43%)]\t0.345800\n",
      "Train Epoch: 2 [28000/60000 (47%)]\t0.474855\n",
      "Train Epoch: 2 [30000/60000 (50%)]\t0.253762\n",
      "Train Epoch: 2 [32000/60000 (53%)]\t0.367632\n",
      "Train Epoch: 2 [34000/60000 (57%)]\t0.193843\n",
      "Train Epoch: 2 [36000/60000 (60%)]\t0.281657\n",
      "Train Epoch: 2 [38000/60000 (63%)]\t0.211389\n",
      "Train Epoch: 2 [40000/60000 (67%)]\t0.490124\n",
      "Train Epoch: 2 [42000/60000 (70%)]\t0.401072\n",
      "Train Epoch: 2 [44000/60000 (73%)]\t0.266973\n",
      "Train Epoch: 2 [46000/60000 (77%)]\t0.291934\n",
      "Train Epoch: 2 [48000/60000 (80%)]\t0.301928\n",
      "Train Epoch: 2 [50000/60000 (83%)]\t0.340863\n",
      "Train Epoch: 2 [52000/60000 (87%)]\t0.267997\n",
      "Train Epoch: 2 [54000/60000 (90%)]\t0.322463\n",
      "Train Epoch: 2 [56000/60000 (93%)]\t0.173806\n",
      "Train Epoch: 2 [58000/60000 (97%)]\t0.326312\n",
      "\n",
      "Test set: Average loss: 0.0009, Accuracy: 9726/10000 (97%\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\t0.244490\n",
      "Train Epoch: 3 [2000/60000 (3%)]\t0.349602\n",
      "Train Epoch: 3 [4000/60000 (7%)]\t0.457251\n",
      "Train Epoch: 3 [6000/60000 (10%)]\t0.310712\n",
      "Train Epoch: 3 [8000/60000 (13%)]\t0.296602\n",
      "Train Epoch: 3 [10000/60000 (17%)]\t0.348298\n",
      "Train Epoch: 3 [12000/60000 (20%)]\t0.439650\n",
      "Train Epoch: 3 [14000/60000 (23%)]\t0.299680\n",
      "Train Epoch: 3 [16000/60000 (27%)]\t0.238775\n",
      "Train Epoch: 3 [18000/60000 (30%)]\t0.236888\n",
      "Train Epoch: 3 [20000/60000 (33%)]\t0.239046\n",
      "Train Epoch: 3 [22000/60000 (37%)]\t0.299768\n",
      "Train Epoch: 3 [24000/60000 (40%)]\t0.161372\n",
      "Train Epoch: 3 [26000/60000 (43%)]\t0.345525\n",
      "Train Epoch: 3 [28000/60000 (47%)]\t0.272109\n",
      "Train Epoch: 3 [30000/60000 (50%)]\t0.293810\n",
      "Train Epoch: 3 [32000/60000 (53%)]\t0.249997\n",
      "Train Epoch: 3 [34000/60000 (57%)]\t0.411545\n",
      "Train Epoch: 3 [36000/60000 (60%)]\t0.373898\n",
      "Train Epoch: 3 [38000/60000 (63%)]\t0.232250\n",
      "Train Epoch: 3 [40000/60000 (67%)]\t0.083444\n",
      "Train Epoch: 3 [42000/60000 (70%)]\t0.452363\n",
      "Train Epoch: 3 [44000/60000 (73%)]\t0.179671\n",
      "Train Epoch: 3 [46000/60000 (77%)]\t0.163894\n",
      "Train Epoch: 3 [48000/60000 (80%)]\t0.401797\n",
      "Train Epoch: 3 [50000/60000 (83%)]\t0.270622\n",
      "Train Epoch: 3 [52000/60000 (87%)]\t0.295123\n",
      "Train Epoch: 3 [54000/60000 (90%)]\t0.335820\n",
      "Train Epoch: 3 [56000/60000 (93%)]\t0.247654\n",
      "Train Epoch: 3 [58000/60000 (97%)]\t0.316014\n",
      "\n",
      "Test set: Average loss: 0.0009, Accuracy: 9732/10000 (97%\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\t0.492496\n",
      "Train Epoch: 4 [2000/60000 (3%)]\t0.288665\n",
      "Train Epoch: 4 [4000/60000 (7%)]\t0.341459\n",
      "Train Epoch: 4 [6000/60000 (10%)]\t0.264031\n",
      "Train Epoch: 4 [8000/60000 (13%)]\t0.220248\n",
      "Train Epoch: 4 [10000/60000 (17%)]\t0.214617\n",
      "Train Epoch: 4 [12000/60000 (20%)]\t0.264404\n",
      "Train Epoch: 4 [14000/60000 (23%)]\t0.323917\n",
      "Train Epoch: 4 [16000/60000 (27%)]\t0.224429\n",
      "Train Epoch: 4 [18000/60000 (30%)]\t0.229440\n",
      "Train Epoch: 4 [20000/60000 (33%)]\t0.269162\n",
      "Train Epoch: 4 [22000/60000 (37%)]\t0.339331\n",
      "Train Epoch: 4 [24000/60000 (40%)]\t0.418086\n",
      "Train Epoch: 4 [26000/60000 (43%)]\t0.267746\n",
      "Train Epoch: 4 [28000/60000 (47%)]\t0.186387\n",
      "Train Epoch: 4 [30000/60000 (50%)]\t0.087558\n",
      "Train Epoch: 4 [32000/60000 (53%)]\t0.137307\n",
      "Train Epoch: 4 [34000/60000 (57%)]\t0.295685\n",
      "Train Epoch: 4 [36000/60000 (60%)]\t0.113279\n",
      "Train Epoch: 4 [38000/60000 (63%)]\t0.228466\n",
      "Train Epoch: 4 [40000/60000 (67%)]\t0.207152\n",
      "Train Epoch: 4 [42000/60000 (70%)]\t0.372529\n",
      "Train Epoch: 4 [44000/60000 (73%)]\t0.381473\n",
      "Train Epoch: 4 [46000/60000 (77%)]\t0.174786\n",
      "Train Epoch: 4 [48000/60000 (80%)]\t0.306766\n",
      "Train Epoch: 4 [50000/60000 (83%)]\t0.138182\n",
      "Train Epoch: 4 [52000/60000 (87%)]\t0.214757\n",
      "Train Epoch: 4 [54000/60000 (90%)]\t0.218632\n",
      "Train Epoch: 4 [56000/60000 (93%)]\t0.167091\n",
      "Train Epoch: 4 [58000/60000 (97%)]\t0.408269\n",
      "\n",
      "Test set: Average loss: 0.0009, Accuracy: 9729/10000 (97%\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\t0.261410\n",
      "Train Epoch: 5 [2000/60000 (3%)]\t0.221175\n",
      "Train Epoch: 5 [4000/60000 (7%)]\t0.172989\n",
      "Train Epoch: 5 [6000/60000 (10%)]\t0.480499\n",
      "Train Epoch: 5 [8000/60000 (13%)]\t0.322233\n",
      "Train Epoch: 5 [10000/60000 (17%)]\t0.205419\n",
      "Train Epoch: 5 [12000/60000 (20%)]\t0.286730\n",
      "Train Epoch: 5 [14000/60000 (23%)]\t0.221861\n",
      "Train Epoch: 5 [16000/60000 (27%)]\t0.387543\n",
      "Train Epoch: 5 [18000/60000 (30%)]\t0.276468\n",
      "Train Epoch: 5 [20000/60000 (33%)]\t0.299445\n",
      "Train Epoch: 5 [22000/60000 (37%)]\t0.668416\n",
      "Train Epoch: 5 [24000/60000 (40%)]\t0.336606\n",
      "Train Epoch: 5 [26000/60000 (43%)]\t0.456844\n",
      "Train Epoch: 5 [28000/60000 (47%)]\t0.217981\n",
      "Train Epoch: 5 [30000/60000 (50%)]\t0.364644\n",
      "Train Epoch: 5 [32000/60000 (53%)]\t0.473067\n",
      "Train Epoch: 5 [34000/60000 (57%)]\t0.319676\n",
      "Train Epoch: 5 [36000/60000 (60%)]\t0.169185\n",
      "Train Epoch: 5 [38000/60000 (63%)]\t0.224205\n",
      "Train Epoch: 5 [40000/60000 (67%)]\t0.220410\n",
      "Train Epoch: 5 [42000/60000 (70%)]\t0.185363\n",
      "Train Epoch: 5 [44000/60000 (73%)]\t0.367512\n",
      "Train Epoch: 5 [46000/60000 (77%)]\t0.337110\n",
      "Train Epoch: 5 [48000/60000 (80%)]\t0.390617\n",
      "Train Epoch: 5 [50000/60000 (83%)]\t0.343665\n",
      "Train Epoch: 5 [52000/60000 (87%)]\t0.322082\n",
      "Train Epoch: 5 [54000/60000 (90%)]\t0.289377\n",
      "Train Epoch: 5 [56000/60000 (93%)]\t0.197380\n",
      "Train Epoch: 5 [58000/60000 (97%)]\t0.301809\n",
      "\n",
      "Test set: Average loss: 0.0007, Accuracy: 9771/10000 (98%\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\t0.199763\n",
      "Train Epoch: 6 [2000/60000 (3%)]\t0.227880\n",
      "Train Epoch: 6 [4000/60000 (7%)]\t0.290355\n",
      "Train Epoch: 6 [6000/60000 (10%)]\t0.099798\n",
      "Train Epoch: 6 [8000/60000 (13%)]\t0.509195\n",
      "Train Epoch: 6 [10000/60000 (17%)]\t0.303386\n",
      "Train Epoch: 6 [12000/60000 (20%)]\t0.534857\n",
      "Train Epoch: 6 [14000/60000 (23%)]\t0.237785\n",
      "Train Epoch: 6 [16000/60000 (27%)]\t0.201154\n",
      "Train Epoch: 6 [18000/60000 (30%)]\t0.276688\n",
      "Train Epoch: 6 [20000/60000 (33%)]\t0.155430\n",
      "Train Epoch: 6 [22000/60000 (37%)]\t0.199922\n",
      "Train Epoch: 6 [24000/60000 (40%)]\t0.301813\n",
      "Train Epoch: 6 [26000/60000 (43%)]\t0.231763\n",
      "Train Epoch: 6 [28000/60000 (47%)]\t0.228213\n",
      "Train Epoch: 6 [30000/60000 (50%)]\t0.413493\n",
      "Train Epoch: 6 [32000/60000 (53%)]\t0.501409\n",
      "Train Epoch: 6 [34000/60000 (57%)]\t0.295128\n",
      "Train Epoch: 6 [36000/60000 (60%)]\t0.230386\n",
      "Train Epoch: 6 [38000/60000 (63%)]\t0.284130\n",
      "Train Epoch: 6 [40000/60000 (67%)]\t0.286576\n",
      "Train Epoch: 6 [42000/60000 (70%)]\t0.257161\n",
      "Train Epoch: 6 [44000/60000 (73%)]\t0.216944\n",
      "Train Epoch: 6 [46000/60000 (77%)]\t0.279864\n",
      "Train Epoch: 6 [48000/60000 (80%)]\t0.221432\n",
      "Train Epoch: 6 [50000/60000 (83%)]\t0.382111\n",
      "Train Epoch: 6 [52000/60000 (87%)]\t0.125057\n",
      "Train Epoch: 6 [54000/60000 (90%)]\t0.214915\n",
      "Train Epoch: 6 [56000/60000 (93%)]\t0.409243\n",
      "Train Epoch: 6 [58000/60000 (97%)]\t0.310400\n",
      "\n",
      "Test set: Average loss: 0.0008, Accuracy: 9767/10000 (98%\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\t0.193301\n",
      "Train Epoch: 7 [2000/60000 (3%)]\t0.336570\n",
      "Train Epoch: 7 [4000/60000 (7%)]\t0.296189\n",
      "Train Epoch: 7 [6000/60000 (10%)]\t0.307566\n",
      "Train Epoch: 7 [8000/60000 (13%)]\t0.142055\n",
      "Train Epoch: 7 [10000/60000 (17%)]\t0.416663\n",
      "Train Epoch: 7 [12000/60000 (20%)]\t0.197925\n",
      "Train Epoch: 7 [14000/60000 (23%)]\t0.173687\n",
      "Train Epoch: 7 [16000/60000 (27%)]\t0.216530\n",
      "Train Epoch: 7 [18000/60000 (30%)]\t0.419565\n",
      "Train Epoch: 7 [20000/60000 (33%)]\t0.158706\n",
      "Train Epoch: 7 [22000/60000 (37%)]\t0.307992\n",
      "Train Epoch: 7 [24000/60000 (40%)]\t0.334615\n",
      "Train Epoch: 7 [26000/60000 (43%)]\t0.322862\n",
      "Train Epoch: 7 [28000/60000 (47%)]\t0.183400\n",
      "Train Epoch: 7 [30000/60000 (50%)]\t0.224722\n",
      "Train Epoch: 7 [32000/60000 (53%)]\t0.258421\n",
      "Train Epoch: 7 [34000/60000 (57%)]\t0.258418\n",
      "Train Epoch: 7 [36000/60000 (60%)]\t0.474390\n",
      "Train Epoch: 7 [38000/60000 (63%)]\t0.193661\n",
      "Train Epoch: 7 [40000/60000 (67%)]\t0.170212\n",
      "Train Epoch: 7 [42000/60000 (70%)]\t0.163020\n",
      "Train Epoch: 7 [44000/60000 (73%)]\t0.343662\n",
      "Train Epoch: 7 [46000/60000 (77%)]\t0.360913\n",
      "Train Epoch: 7 [48000/60000 (80%)]\t0.416101\n",
      "Train Epoch: 7 [50000/60000 (83%)]\t0.287381\n",
      "Train Epoch: 7 [52000/60000 (87%)]\t0.158945\n",
      "Train Epoch: 7 [54000/60000 (90%)]\t0.216483\n",
      "Train Epoch: 7 [56000/60000 (93%)]\t0.193045\n",
      "Train Epoch: 7 [58000/60000 (97%)]\t0.305244\n",
      "\n",
      "Test set: Average loss: 0.0008, Accuracy: 9770/10000 (98%\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\t0.219496\n",
      "Train Epoch: 8 [2000/60000 (3%)]\t0.426131\n",
      "Train Epoch: 8 [4000/60000 (7%)]\t0.323429\n",
      "Train Epoch: 8 [6000/60000 (10%)]\t0.160395\n",
      "Train Epoch: 8 [8000/60000 (13%)]\t0.355431\n",
      "Train Epoch: 8 [10000/60000 (17%)]\t0.438350\n",
      "Train Epoch: 8 [12000/60000 (20%)]\t0.175095\n",
      "Train Epoch: 8 [14000/60000 (23%)]\t0.182487\n",
      "Train Epoch: 8 [16000/60000 (27%)]\t0.183004\n",
      "Train Epoch: 8 [18000/60000 (30%)]\t0.196250\n",
      "Train Epoch: 8 [20000/60000 (33%)]\t0.180062\n",
      "Train Epoch: 8 [22000/60000 (37%)]\t0.287033\n",
      "Train Epoch: 8 [24000/60000 (40%)]\t0.248617\n",
      "Train Epoch: 8 [26000/60000 (43%)]\t0.224189\n",
      "Train Epoch: 8 [28000/60000 (47%)]\t0.174473\n",
      "Train Epoch: 8 [30000/60000 (50%)]\t0.157526\n",
      "Train Epoch: 8 [32000/60000 (53%)]\t0.275688\n",
      "Train Epoch: 8 [34000/60000 (57%)]\t0.474908\n",
      "Train Epoch: 8 [36000/60000 (60%)]\t0.285161\n",
      "Train Epoch: 8 [38000/60000 (63%)]\t0.213791\n",
      "Train Epoch: 8 [40000/60000 (67%)]\t0.234656\n",
      "Train Epoch: 8 [42000/60000 (70%)]\t0.328727\n",
      "Train Epoch: 8 [44000/60000 (73%)]\t0.283318\n",
      "Train Epoch: 8 [46000/60000 (77%)]\t0.312357\n",
      "Train Epoch: 8 [48000/60000 (80%)]\t0.268953\n",
      "Train Epoch: 8 [50000/60000 (83%)]\t0.345380\n",
      "Train Epoch: 8 [52000/60000 (87%)]\t0.149247\n",
      "Train Epoch: 8 [54000/60000 (90%)]\t0.272303\n",
      "Train Epoch: 8 [56000/60000 (93%)]\t0.177671\n",
      "Train Epoch: 8 [58000/60000 (97%)]\t0.179818\n",
      "\n",
      "Test set: Average loss: 0.0008, Accuracy: 9777/10000 (98%\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\t0.495081\n",
      "Train Epoch: 9 [2000/60000 (3%)]\t0.240817\n",
      "Train Epoch: 9 [4000/60000 (7%)]\t0.265638\n",
      "Train Epoch: 9 [6000/60000 (10%)]\t0.420329\n",
      "Train Epoch: 9 [8000/60000 (13%)]\t0.243039\n",
      "Train Epoch: 9 [10000/60000 (17%)]\t0.376234\n",
      "Train Epoch: 9 [12000/60000 (20%)]\t0.354393\n",
      "Train Epoch: 9 [14000/60000 (23%)]\t0.440228\n",
      "Train Epoch: 9 [16000/60000 (27%)]\t0.250073\n",
      "Train Epoch: 9 [18000/60000 (30%)]\t0.243008\n",
      "Train Epoch: 9 [20000/60000 (33%)]\t0.344634\n",
      "Train Epoch: 9 [22000/60000 (37%)]\t0.249147\n",
      "Train Epoch: 9 [24000/60000 (40%)]\t0.182881\n",
      "Train Epoch: 9 [26000/60000 (43%)]\t0.149785\n",
      "Train Epoch: 9 [28000/60000 (47%)]\t0.137327\n",
      "Train Epoch: 9 [30000/60000 (50%)]\t0.328159\n",
      "Train Epoch: 9 [32000/60000 (53%)]\t0.201738\n",
      "Train Epoch: 9 [34000/60000 (57%)]\t0.210465\n",
      "Train Epoch: 9 [36000/60000 (60%)]\t0.322404\n",
      "Train Epoch: 9 [38000/60000 (63%)]\t0.150809\n",
      "Train Epoch: 9 [40000/60000 (67%)]\t0.251630\n",
      "Train Epoch: 9 [42000/60000 (70%)]\t0.256732\n",
      "Train Epoch: 9 [44000/60000 (73%)]\t0.178838\n",
      "Train Epoch: 9 [46000/60000 (77%)]\t0.138947\n",
      "Train Epoch: 9 [48000/60000 (80%)]\t0.322768\n",
      "Train Epoch: 9 [50000/60000 (83%)]\t0.177701\n",
      "Train Epoch: 9 [52000/60000 (87%)]\t0.281634\n",
      "Train Epoch: 9 [54000/60000 (90%)]\t0.166784\n",
      "Train Epoch: 9 [56000/60000 (93%)]\t0.313611\n",
      "Train Epoch: 9 [58000/60000 (97%)]\t0.259514\n",
      "\n",
      "Test set: Average loss: 0.0008, Accuracy: 9775/10000 (98%\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\t0.178863\n",
      "Train Epoch: 10 [2000/60000 (3%)]\t0.205111\n",
      "Train Epoch: 10 [4000/60000 (7%)]\t0.245849\n",
      "Train Epoch: 10 [6000/60000 (10%)]\t0.319531\n",
      "Train Epoch: 10 [8000/60000 (13%)]\t0.167533\n",
      "Train Epoch: 10 [10000/60000 (17%)]\t0.315555\n",
      "Train Epoch: 10 [12000/60000 (20%)]\t0.559016\n",
      "Train Epoch: 10 [14000/60000 (23%)]\t0.323355\n",
      "Train Epoch: 10 [16000/60000 (27%)]\t0.311168\n",
      "Train Epoch: 10 [18000/60000 (30%)]\t0.490431\n",
      "Train Epoch: 10 [20000/60000 (33%)]\t0.274231\n",
      "Train Epoch: 10 [22000/60000 (37%)]\t0.268786\n",
      "Train Epoch: 10 [24000/60000 (40%)]\t0.417311\n",
      "Train Epoch: 10 [26000/60000 (43%)]\t0.213271\n",
      "Train Epoch: 10 [28000/60000 (47%)]\t0.234426\n",
      "Train Epoch: 10 [30000/60000 (50%)]\t0.250061\n",
      "Train Epoch: 10 [32000/60000 (53%)]\t0.298397\n",
      "Train Epoch: 10 [34000/60000 (57%)]\t0.297481\n",
      "Train Epoch: 10 [36000/60000 (60%)]\t0.335258\n",
      "Train Epoch: 10 [38000/60000 (63%)]\t0.158989\n",
      "Train Epoch: 10 [40000/60000 (67%)]\t0.525303\n",
      "Train Epoch: 10 [42000/60000 (70%)]\t0.223812\n",
      "Train Epoch: 10 [44000/60000 (73%)]\t0.180107\n",
      "Train Epoch: 10 [46000/60000 (77%)]\t0.408550\n",
      "Train Epoch: 10 [48000/60000 (80%)]\t0.195239\n",
      "Train Epoch: 10 [50000/60000 (83%)]\t0.164448\n",
      "Train Epoch: 10 [52000/60000 (87%)]\t0.065565\n",
      "Train Epoch: 10 [54000/60000 (90%)]\t0.129851\n",
      "Train Epoch: 10 [56000/60000 (93%)]\t0.326899\n",
      "Train Epoch: 10 [58000/60000 (97%)]\t0.163414\n",
      "\n",
      "Test set: Average loss: 0.0007, Accuracy: 9783/10000 (98%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,11):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79bd2f33-0171-47d8-a1c9-2df70b19481d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6e59ed-b9b4-4584-adf1-01c10a9d3416",
   "metadata": {},
   "source": [
    "This code snippet is used to make a prediction with a trained neural network model and visualize the input image along with the model's prediction. Let's break down the key components:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "\n",
    "- This line imports the `matplotlib.pyplot` module, which is commonly used for creating visualizations, such as plots and images.\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "- Sets the model to evaluation mode. This is necessary when you want to make predictions or evaluate the model on a test set. It ensures that layers like dropout behave differently during evaluation compared to training.\n",
    "\n",
    "```python\n",
    "data, target = test_data[2]\n",
    "```\n",
    "\n",
    "- Retrieves an input image (`data`) and its corresponding target label (`target`) from the test dataset (`test_data`). The index 2 is used as an example, and you can replace it with any other index to visualize a different image.\n",
    "\n",
    "```python\n",
    "data = data.unsqueeze(0).to(device)\n",
    "```\n",
    "\n",
    "- Prepares the input data for the model by adding a batch dimension (`unsqueeze(0)`) and moving it to the specified device (either GPU or CPU).\n",
    "\n",
    "```python\n",
    "output = model(data)\n",
    "```\n",
    "\n",
    "- Performs a forward pass through the model to obtain the output. The `output` tensor contains the model's prediction for the input image.\n",
    "\n",
    "```python\n",
    "prediction = output.argmax(dim=1, keepdim=True).item()\n",
    "```\n",
    "\n",
    "- Determines the predicted class by finding the index of the maximum value along dimension 1. The result is a scalar value (`item()`) representing the predicted class.\n",
    "\n",
    "```python\n",
    "print(f\"Prediction: {prediction}\")\n",
    "```\n",
    "\n",
    "- Prints the predicted class to the console.\n",
    "\n",
    "```python\n",
    "image = data.squeeze(0).squeeze(0).cpu().numpy()\n",
    "```\n",
    "\n",
    "- Prepares the image for visualization by removing the added batch dimension (`squeeze(0)`) and converting the PyTorch tensor to a NumPy array on the CPU.\n",
    "\n",
    "```python\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "- Uses Matplotlib to display the image. The `cmap='gray'` argument indicates that the image is grayscale. The `plt.show()` command is used to display the image in a pop-up window.\n",
    "\n",
    "In summary, this code snippet loads an image from the test dataset, passes it through a trained neural network model, prints the predicted class, and visualizes the image using Matplotlib. It's a useful way to inspect how well the model is performing on individual examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e1b27ba-edc2-4813-b8f5-24e5e812da1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model.eval()\n",
    "data,target = test_data[2]\n",
    "data = data.unsqueeze(0).to(device)\n",
    "output = model(data)\n",
    "prediction = output.argmax(dim=1,keepdim = True).item()\n",
    "print(f\"Prediction: {prediction}\")\n",
    "image = data.squeeze(0).squeeze(0).cpu().numpy()\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1903a2-47ca-4777-b7f8-ddc3dd9d98b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af2f308-13ac-4379-9faa-802c39a37f62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4419618b-16c8-4aaa-b629-0ef044c2ed02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b148fe8-1b4e-4351-b9e5-3a150d1d2ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57cb43b-e2fe-448a-b388-f75d09e706da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef60e25-07a3-4022-b566-7ab5b6542cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa61179-b86b-4022-b2b4-dc8ef194d881",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
